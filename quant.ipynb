{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e65830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from tf_keras.layers import Input,Dense\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,Conv2DTranspose,concatenate\n",
    "from tf_keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "from tf_keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "import tf_keras.backend as K\n",
    "from tf_keras.losses import categorical_crossentropy\n",
    "from tf_keras.preprocessing.image import ImageDataGenerator\n",
    "from tf_keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_keras.layers import Input,Dense\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,Conv2DTranspose,Conv3D,DepthwiseConv2D,DepthwiseConv1D\n",
    "from tf_keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "from tf_keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "import tf_keras.backend as K\n",
    "from tf_keras.preprocessing.image import ImageDataGenerator\n",
    "from tf_keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tf_keras.applications.mobilenet_v2 import MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filename = glob.glob(r'DeepLearningBasedTBDiagnosis/dataset/TBX11K/imgs/tb/*.*')\n",
    "print(len(filename))\n",
    "filename = glob.glob(r'DeepLearningBasedTBDiagnosis/dataset/TBX11K/imgs/sick/*.*')\n",
    "print(len(filename))\n",
    "filename = glob.glob(r'DeepLearningBasedTBDiagnosis/dataset/TBX11K/imgs/health/*.*')\n",
    "print(len(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb25692",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (256,256)\n",
    "\n",
    "train_dir = 'dataset/TBX11K/imgs/train/'\n",
    "test_dir = 'dataset/TBX11K/imgs/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import define_data\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "train = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import define_data\n",
    "train_data, train_y = define_data(train_dir,IMG_SIZE)\n",
    "target_val = train.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d80ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.15\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(train_data, ),target_val,test_size=test_size,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_net = MobileNetV2\n",
    "net = \"MobileNetV2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    inputs = Input(shape=input_shape,name=\"input_image\")\n",
    "    base_model = back_net(input_tensor = inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n",
    "    #base_model.trainable=False\n",
    "    x=base_model.output\n",
    "    x=GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "    output=Dense(3,activation=\"softmax\")(x)\n",
    "    return tf_keras.Model(inputs,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e245a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the steps of this are vaguely unclear to me - i.e., do I need to compile again? is load_weights sufficient? time to research\n",
    "base_model = model(input_shape=(256,256,3))\n",
    "base_model.load_weights('mobilnet-output/class_weights.06-0.96.weights.h5')\n",
    "base_model.compile(optimizer=tf_keras.optimizers.Adam(),\n",
    "              loss=tf_keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "#test_size exists\n",
    "image_num = x_train.shape[0] * (1-test_size)\n",
    "end_step = np.ceil(image_num / batch_size).astype(np.int32) * epochs\n",
    "print(image_num, end_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=f'{back_net.__name__}/pruned/class_weights.{{epoch:02d}}-{{val_loss:.2f}}.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18440ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf_keras.callbacks.ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        save_best_only=True,save_weights_only=True,verbose=1),\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir=\"./logs\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24594f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ada5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(teacher)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file=f'{back_net.__name__}.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(teacher)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "quantized_and_pruned_tflite_file=f'{back_net.__name__}.tflite'\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=quantized_pruned_tflite_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details=interpreter.get_input_details()\n",
    "output_details=interpreter.get_output_details()\n",
    "print(input_details,output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tflite_predict(image):\n",
    "    interpreter.set_tensor(input_details[0][\"index\"],image[np.newaxis,...])\n",
    "    interpreter.invoke()\n",
    "    return interpreter.get_tensor(output_details[0][\"index\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95930fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = x_test[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ed3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = len(x_test)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(total):\n",
    "    preds = tflite_predict(x_test[i])\n",
    "    y_pred.append(np.argmax(preds))\n",
    "    y_true.append(np.argmax(y_test[i]))\n",
    "\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred,target_names=train.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24315281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true,y_pred,labels=[0,1,2])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[\"NORM\",\"NON-TB\",\"TB\"])\n",
    "disp.plot()\n",
    "plt.savefig(f\"{back_net.__name__}prunequanttflite-confusionmatrix.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
